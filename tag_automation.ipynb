{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests bs4 selenium time random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    StaleElementReferenceException,\n",
    ")\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_css_selectors(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    css_selectors = []\n",
    "\n",
    "    def extract_selectors(element):\n",
    "        tag = element.name\n",
    "        classes = element.get(\"class\")\n",
    "        class_selector = f\".{'.'.join(classes)}\" if classes else None\n",
    "        id_selector = f\"#{element.get('id')}\" if element.get(\"id\") else None\n",
    "\n",
    "        if class_selector:\n",
    "            css_selectors.append(class_selector)\n",
    "        if id_selector:\n",
    "            css_selectors.append(id_selector)\n",
    "        if tag:\n",
    "            css_selectors.append(tag)\n",
    "\n",
    "        for child in element.children:\n",
    "            if isinstance(child, BeautifulSoup) or isinstance(child, str):\n",
    "                continue\n",
    "            extract_selectors(child)\n",
    "\n",
    "    extract_selectors(soup)\n",
    "    return list(set(css_selectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Send a query to local Ollama server\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Error communicating with Ollama: {str(e)}\")\n",
    "    \n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract the first JSON object from text\"\"\"\n",
    "    try:\n",
    "        # First try to parse the entire text as JSON\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # If that fails, try to find JSON object in the text\n",
    "        try:\n",
    "            # Find the first occurrence of a JSON-like structure\n",
    "            start_idx = text.find('{')\n",
    "            end_idx = text.rfind('}') + 1\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = text[start_idx:end_idx]\n",
    "                return json.loads(json_str)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            # If still no valid JSON, try cleaning the text\n",
    "            cleaned_text = re.sub(r'```json|```', '', text)  # Remove markdown code blocks\n",
    "            cleaned_text = cleaned_text.strip()\n",
    "            try:\n",
    "                return json.loads(cleaned_text)\n",
    "            except json.JSONDecodeError:\n",
    "                raise Exception(\"Could not extract valid JSON from response\")\n",
    "\n",
    "def get_tag_suggestions(css_selectors):\n",
    "    # Create a prompt that asks the model to identify relevant selectors\n",
    "    prompt = f\"\"\"Given these CSS selectors from a website:\n",
    "{json.dumps(css_selectors, indent=2)}\n",
    "\n",
    "Analyze them and identify the most likely selectors for:\n",
    "1. Review text container\n",
    "2. Author name\n",
    "3. Rating element\n",
    "4. Next page button\n",
    "\n",
    "Common patterns to look for:\n",
    "- Review containers often have classes/IDs with words like 'review', 'comment', 'text', 'content'\n",
    "- Author elements often contain 'author', 'user', 'name', 'by'\n",
    "- Rating elements usually have 'rating', 'stars', 'score'\n",
    "- Next page buttons typically include 'next', 'pagination', 'nav'\n",
    "\n",
    "Return ONLY a JSON object in this exact format (no other text):\n",
    "{{\n",
    "    \"review_tag\": \"selector_for_review_container\",\n",
    "    \"author_tag\": \"selector_for_author_name\",\n",
    "    \"rating_tag\": \"selector_for_rating\",\n",
    "    \"next_page_button_tag\": \"selector_for_next_button\"\n",
    "}}\"\"\"\n",
    "\n",
    "    # Get response from Ollama\n",
    "    response = query_ollama(prompt)\n",
    "    \n",
    "    return extract_json_from_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dynamic_popups(driver):\n",
    "    try:\n",
    "        potential_popups = driver.find_elements(By.CSS_SELECTOR, \"*\")\n",
    "        for element in potential_popups:\n",
    "            try:\n",
    "                style = element.get_attribute(\"style\")\n",
    "                if \"z-index\" in style and \"visibility: visible\" in style:\n",
    "                    close_buttons = element.find_elements(By.CSS_SELECTOR, \"button, a, span\")\n",
    "                    for button in close_buttons:\n",
    "                        if button.is_displayed() and button.is_enabled():\n",
    "                            button.click()\n",
    "                            time.sleep(1)\n",
    "                            return True\n",
    "                    action = ActionChains(driver)\n",
    "                    random_x = random.randint(0, driver.execute_script(\"return window.innerWidth;\"))\n",
    "                    random_y = random.randint(0, driver.execute_script(\"return window.innerHeight;\"))\n",
    "                    action.move_by_offset(random_x, random_y).click().perform()\n",
    "                    time.sleep(1)\n",
    "                    return True\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_reviews(url, review_tag, author_tag, rating_tag, next_page_button_tag):\n",
    "    # Set up Selenium WebDriver\n",
    "    service = Service(\"chromedriver.exe\")  # Update this path\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.maximize_window()  # Maximize window for better visibility\n",
    "\n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)  # Wait for elements to load\n",
    "\n",
    "        all_reviews = []\n",
    "        page_number = 1  # Track the current page number for debugging\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Wait for the reviews section to load\n",
    "                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, review_tag)))\n",
    "\n",
    "                # Get the current page source and parse it with BeautifulSoup\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                # Find all review elements using the passed review_tag, author_tag, and rating_tag\n",
    "                review_elements = soup.select(review_tag)\n",
    "                author_elements = soup.select(author_tag)\n",
    "                rating_elements = soup.select(rating_tag)\n",
    "\n",
    "                # Extract reviews, authors, and ratings\n",
    "                for review, author, rating in zip(review_elements, author_elements, rating_elements):\n",
    "                    review_text = review.get_text(strip=True)\n",
    "                    author_name = author.get_text(strip=True)\n",
    "                    rating_score = rating.get(\"data-score\", \"N/A\")  # Extract 'data-score' or use 'N/A' if missing\n",
    "                    all_reviews.append({\n",
    "                        \"review\": review_text,\n",
    "                        \"author\": author_name,\n",
    "                        \"rating\": rating_score\n",
    "                    })\n",
    "\n",
    "                print(f\"Page {page_number} scraped successfully with {len(review_elements)} reviews.\")\n",
    "                page_number += 1\n",
    "\n",
    "                # Scroll the \"Next Page\" button into view and click it\n",
    "                try:\n",
    "                    next_page_button = driver.find_element(By.CSS_SELECTOR, next_page_button_tag)\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_page_button)\n",
    "                    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, next_page_button_tag))).click()\n",
    "                    time.sleep(2)  # Small pause to ensure the page loads\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    print(\"No more pages to scrape or 'Next Page' button not found.\")\n",
    "                    break\n",
    "                except ElementClickInterceptedException as e:\n",
    "                    print(f\"Click intercepted on page {page_number}: {str(e)}\")\n",
    "                    ActionChains(driver).move_to_element_with_offset(next_page_button, 0, 0).click().perform()\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page_number}: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        # Ensure the browser is closed properly\n",
    "        driver.quit()\n",
    "\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running this script, make sure:\n",
      "1. Ollama is installed (https://ollama.ai/download)\n",
      "2. You've pulled the llama3.2 model: Run 'ollama pull llama3.2'\n",
      "3. Ollama server is running: Run 'ollama serve' in a terminal\n",
      "\n",
      "Press Enter when ready, or Ctrl+C to exit\n",
      "Fetching CSS selectors from the page...\n",
      "Found 315 unique CSS selectors\n",
      "\n",
      "Analyzing selectors with Ollama...\n",
      "\n",
      "Ollama suggested these selectors:\n",
      "{\n",
      "  \"review_tag\": \".jdgm-rev-widg__content\",\n",
      "  \"author_tag\": \".jdgm-rev__author\",\n",
      "  \"rating_tag\": \".jdgm-rev__rating\",\n",
      "  \"next_page_button_tag\": \".pagination\"\n",
      "}\n",
      "\n",
      "Would you like to:\n",
      "1. Proceed with these selectors\n",
      "2. Modify the selectors\n",
      "3. Abort\n",
      "\n",
      "Enter new selectors (press Enter to keep existing one):\n",
      "\n",
      "Starting review collection...\n",
      "Error scraping page 1: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=131.0.6778.265)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6CC2680D5+2992373]\n",
      "\t(No symbol) [0x00007FF6CBEFBFD0]\n",
      "\t(No symbol) [0x00007FF6CBD9590A]\n",
      "\t(No symbol) [0x00007FF6CBD6F4F5]\n",
      "\t(No symbol) [0x00007FF6CBE163A7]\n",
      "\t(No symbol) [0x00007FF6CBE2EE72]\n",
      "\t(No symbol) [0x00007FF6CBE0F113]\n",
      "\t(No symbol) [0x00007FF6CBDDA918]\n",
      "\t(No symbol) [0x00007FF6CBDDBA81]\n",
      "\tGetHandleVerifier [0x00007FF6CC2C6A2D+3379789]\n",
      "\tGetHandleVerifier [0x00007FF6CC2DC32D+3468109]\n",
      "\tGetHandleVerifier [0x00007FF6CC2D0043+3418211]\n",
      "\tGetHandleVerifier [0x00007FF6CC05C78B+847787]\n",
      "\t(No symbol) [0x00007FF6CBF0757F]\n",
      "\t(No symbol) [0x00007FF6CBF02FC4]\n",
      "\t(No symbol) [0x00007FF6CBF0315D]\n",
      "\t(No symbol) [0x00007FF6CBEF2979]\n",
      "\tBaseThreadInitThunk [0x00007FFCCC95259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFCCD6EAF38+40]\n",
      "\n",
      "\n",
      "Collected 0 reviews:\n"
     ]
    }
   ],
   "source": [
    "def main(url):\n",
    "    # Check if Ollama server is running\n",
    "    try:\n",
    "        requests.get(\"http://localhost:11434/api/version\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Error: Cannot connect to Ollama server. Please ensure it's running on localhost:11434\")\n",
    "        return\n",
    "\n",
    "    # First, fetch all CSS selectors from the page\n",
    "    print(\"Fetching CSS selectors from the page...\")\n",
    "    css_selectors = fetch_css_selectors(url)\n",
    "    print(f\"Found {len(css_selectors)} unique CSS selectors\")\n",
    "    \n",
    "    # Get tag suggestions from Ollama\n",
    "    print(\"\\nAnalyzing selectors with Ollama...\")\n",
    "    try:\n",
    "        tag_suggestions = get_tag_suggestions(css_selectors)\n",
    "        print(\"\\nOllama suggested these selectors:\")\n",
    "        print(json.dumps(tag_suggestions, indent=2))\n",
    "        \n",
    "        # Allow user to modify suggestions if needed\n",
    "        print(\"\\nWould you like to:\")\n",
    "        print(\"1. Proceed with these selectors\")\n",
    "        print(\"2. Modify the selectors\")\n",
    "        print(\"3. Abort\")\n",
    "        choice = input(\"Enter your choice (1/2/3): \")\n",
    "        \n",
    "        if choice == \"2\":\n",
    "            print(\"\\nEnter new selectors (press Enter to keep existing one):\")\n",
    "            for key, value in tag_suggestions.items():\n",
    "                new_value = input(f\"{key} [{value}]: \").strip()\n",
    "                if new_value:\n",
    "                    tag_suggestions[key] = new_value\n",
    "        elif choice == \"3\":\n",
    "            print(\"Aborting scraping process.\")\n",
    "            return\n",
    "        elif choice != \"1\":\n",
    "            print(\"Invalid choice. Aborting.\")\n",
    "            return\n",
    "        \n",
    "        # Fetch reviews using the suggested tags\n",
    "        print(\"\\nStarting review collection...\")\n",
    "        reviews = fetch_all_reviews(\n",
    "            url,\n",
    "            tag_suggestions['review_tag'],\n",
    "            tag_suggestions['author_tag'],\n",
    "            tag_suggestions['rating_tag'],\n",
    "            tag_suggestions['next_page_button_tag']\n",
    "        )\n",
    "        \n",
    "        # Print reviews\n",
    "        print(f\"\\nCollected {len(reviews)} reviews:\")\n",
    "        for idx, review in enumerate(reviews, 1):\n",
    "            print(f\"\\nReview {idx}:\")\n",
    "            print(f\"Author: {review['author']}\")\n",
    "            print(f\"Rating: {review['rating']} stars\")\n",
    "            print(f\"Review: {review['review']}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Offer to save reviews to file\n",
    "        save = input(\"\\nWould you like to save the reviews to a file? (y/n): \")\n",
    "        if save.lower() == 'y':\n",
    "            filename = input(\"Enter filename (default: reviews.json): \").strip() or \"reviews.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(reviews, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"Reviews saved to {filename}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"Please check the selectors and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Instructions for setting up Ollama\n",
    "    print(\"Before running this script, make sure:\")\n",
    "    print(\"1. Ollama is installed (https://ollama.ai/download)\")\n",
    "    print(\"2. You've pulled the llama3.2 model: Run 'ollama pull llama3.2'\")\n",
    "    print(\"3. Ollama server is running: Run 'ollama serve' in a terminal\")\n",
    "    print(\"\\nPress Enter when ready, or Ctrl+C to exit\")\n",
    "    input()\n",
    "    \n",
    "    url = input(\"Enter the URL to scrape: \")\n",
    "    main(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
