{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import *\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract all text from the webpage\n",
    "        page_text = soup.get_text()\n",
    "\n",
    "        # Clean up the text by removing excessive whitespace\n",
    "        clean_text = \"\\n\".join(line.strip() for line in page_text.splitlines() if line.strip())\n",
    "\n",
    "        return clean_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract JSON from text response, with multiple fallback methods\"\"\"\n",
    "    # First try: direct JSON parsing\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Second try: find JSON-like structure\n",
    "    try:\n",
    "        json_match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        pass\n",
    "\n",
    "    # Third try: extract individual review objects\n",
    "    try:\n",
    "        reviews = []\n",
    "        matches = re.finditer(r'\\{\\s*\"review\":[^}]+\\}', text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                review = json.loads(match.group())\n",
    "                reviews.append(review)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        if reviews:\n",
    "            return reviews\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    \"\"\"\n",
    "    Scrapes content from a single page.\n",
    "    Returns the HTML content of the page.\n",
    "    \"\"\"\n",
    "    service = Service(\"chromedriver.exe\")  # Update path as needed\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.maximize_window()\n",
    "    page_content = \"\"\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Wait for content to load\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "        # Get page content\n",
    "        page_content = driver.page_source\n",
    "        print(\"Scraped page content successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    print(\"Page Content:\", page_content[:500])\n",
    "\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt):\n",
    "    \"\"\"Send a query to local Ollama server with improved response handling\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Debug: print the response text\n",
    "        print(\"Debug - Ollama raw response:\", response.text)\n",
    "\n",
    "        result = response.json()\n",
    "        if 'response' not in result:\n",
    "            print(\"Debug - Unexpected Ollama response format:\", result)\n",
    "            raise Exception(\"Unexpected response format from Ollama\")\n",
    "\n",
    "        return result['response']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Debug - Ollama API error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_split_text(text, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Splits text into chunks, ensuring JSON structures are not broken.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for line in text.splitlines():\n",
    "        if len(current_chunk) + len(line) + 1 > max_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = line\n",
    "        else:\n",
    "            current_chunk += \"\\n\" + line\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pagination_selector(url):\n",
    "    \"\"\"\n",
    "    Asks Ollama to identify the pagination selector from the page.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Get all elements that might be pagination buttons\n",
    "    potential_elements = soup.find_all(['a', 'button', 'div'], class_=lambda x: x and ('next' in x.lower() or 'pagination' in x.lower()))\n",
    "    selectors = []\n",
    "    for element in potential_elements:\n",
    "        if element.get('class'):\n",
    "            selectors.append(f\".{'.'.join(element.get('class'))}\")\n",
    "        if element.get('id'):\n",
    "            selectors.append(f\"#{element.get('id')}\")\n",
    "    \n",
    "    if not selectors:\n",
    "        return None\n",
    "\n",
    "    prompt = f\"\"\"Analyze these potential pagination selectors and identify which one is most likely the 'Next Page' button:\n",
    "{json.dumps(selectors, indent=2)}\n",
    "\n",
    "\n",
    "\n",
    "Return only the most likely selector as a string, no other text.\"\"\"\n",
    "    try:\n",
    "        response = query_ollama(prompt)\n",
    "        return response.strip().strip('\"').strip(\"'\")\n",
    "    except Exception:\n",
    "        print(\"Debug - Ollama failed to identify the selector. Manual input required.\")\n",
    "        return input(\"Enter the pagination selector manually (CSS format): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews_from_content(page_content):\n",
    "    \"\"\"\n",
    "    Asks Ollama to extract reviews from the page content\n",
    "    \"\"\"\n",
    "    # Clean the HTML content first\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # Get text content\n",
    "    text = soup.get_text()\n",
    "    if not text.strip():\n",
    "        print(\"Debug - Empty text content after cleaning HTML.\")\n",
    "        return []\n",
    "\n",
    "    # Clean up whitespace\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    text = ' '.join(line for line in lines if line)\n",
    "    print(\"Debug - Extracted text:\", text[:500])  # Print first 500 characters\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = \"\"\"Extract all reviews from this webpage content. For each review, identify:\n",
    "    1. The review text\n",
    "    2. The author/username\n",
    "    3. The rating (if present)\n",
    "\n",
    "    Return the results as a JSON array of objects, each with 'review', 'author', and 'rating' fields. Example format:\n",
    "    [\n",
    "        {\n",
    "            \"review\": \"review text here\",\n",
    "            \"author\": \"username here\",\n",
    "            \"rating\": \"5\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    Webpage content:\n",
    "    \"\"\"\n",
    "    print(\"Prepared Prompt for Ollama:\", prompt[:500])  # Check the first 500 characters of the prompt\n",
    "\n",
    "    # Split content into chunks if too long\n",
    "    max_chunk_size = 4000\n",
    "    chunks = safe_split_text(text, max_chunk_size)\n",
    "\n",
    "    all_reviews = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = query_ollama(prompt + chunk)\n",
    "            chunk_reviews = json.loads(response)  # Add a check to verify the response is valid JSON\n",
    "            all_reviews.extend(chunk_reviews)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error processing chunk (JSON decode error): {str(e)}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reviews(reviews, filename=\"reviews.json\"):\n",
    "    \"\"\"\n",
    "    Save the reviews to a JSON file\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(reviews, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     print(\"Before running this script, make sure:\")\n",
    "#     print(\"1. Ollama is installed (https://ollama.ai/download)\")\n",
    "#     print(\"2. You've pulled the llama3.2 model: Run 'ollama pull llama3.2'\")\n",
    "#     print(\"3. Ollama server is running: Run 'ollama serve' in a terminal\")\n",
    "#     print(\"\\nPress Enter when ready, or Ctrl+C to exit\")\n",
    "#     input()\n",
    "\n",
    "#     url = input(\"Enter the URL to scrape: \")\n",
    "\n",
    "#     try:\n",
    "#         # Scrape the page content\n",
    "#         print(\"\\nScraping page content...\")\n",
    "#         page_content = get_page_content(url)\n",
    "\n",
    "#         # Extract reviews from the page\n",
    "#         print(\"\\nExtracting reviews from content...\")\n",
    "#         reviews = extract_reviews_from_content(page_content)\n",
    "\n",
    "#         # Print results\n",
    "#         print(f\"\\nFound {len(reviews)} reviews:\")\n",
    "#         for idx, review in enumerate(reviews, 1):\n",
    "#             print(f\"\\nReview {idx}:\")\n",
    "#             print(f\"Author: {review['author']}\")\n",
    "#             print(f\"Rating: {review.get('rating', 'N/A')} stars\")\n",
    "#             print(f\"Review: {review['review']}\")\n",
    "#             print(\"-\" * 50)\n",
    "\n",
    "#         # Save results\n",
    "#         save = input(\"\\nWould you like to save the reviews to a file? (y/n): \")\n",
    "#         if save.lower() == 'y':\n",
    "#             filename = input(\"Enter filename (default: reviews.json): \").strip() or \"reviews.json\"\n",
    "#             save_reviews(reviews, filename)\n",
    "#             print(f\"Reviews saved to {filename}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#         print(\"Please check the URL and try again.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content extracted successfully.\n",
      "Debug - Extracted text: 27:17 | Relief & Recovery Cream Skip to content 0 Close Back Home Shop Now Our Mission Ambassador Application Wholesale Partnerships Close 0 products in your cart 0 Total: $0.00 Shipping & taxes calculated at checkout View Cart Update cart Checkout          One or more of the items in your cart is a recurring or deferred purchase. By continuing, I agree to the cancellation policy and authorize you to charge my payment method at the prices, frequency and dates listed on this page until my order i\n",
      "Prepared Prompt for Ollama: Extract all reviews from this webpage content. For each review, identify:\n",
      "    1. The review text\n",
      "    2. The author/username\n",
      "    3. The rating (if present)\n",
      "\n",
      "    Return the results as a JSON array of objects, each with 'review', 'author', and 'rating' fields. Example format:\n",
      "    [\n",
      "        {\n",
      "            \"review\": \"review text here\",\n",
      "            \"author\": \"username here\",\n",
      "            \"rating\": \"5\"\n",
      "        }\n",
      "    ]\n",
      "\n",
      "    Webpage content:\n",
      "    \n",
      "Debug - Ollama raw response: {\"model\":\"llama3.2\",\"created_at\":\"2025-01-16T18:23:26.8084973Z\",\"response\":\"I'm happy to help you with the review extraction, but it seems like there's a missing webpage content. Could you please provide the content of the webpage? I'll be happy to extract the reviews and format them according to your requirements.\\n\\nIf you don't have the webpage content, you can also paste a small snippet or describe the content (e.g., product name, platform, etc.) so I can try to help you with a general solution.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,30059,682,8544,505,420,45710,2262,13,1789,1855,3477,11,10765,512,262,220,16,13,578,3477,1495,198,262,220,17,13,578,3229,14,5223,198,262,220,18,13,578,10959,320,333,3118,696,262,3494,279,3135,439,264,4823,1358,315,6302,11,1855,449,364,19981,518,364,3170,518,323,364,22696,6,5151,13,13688,3645,512,262,2330,286,341,310,330,19981,794,330,19981,1495,1618,761,310,330,3170,794,330,5223,1618,761,310,330,22696,794,330,20,702,286,457,262,10661,262,5000,2964,2262,512,257,128009,128006,78191,128007,271,40,2846,6380,311,1520,499,449,279,3477,33289,11,719,433,5084,1093,1070,596,264,7554,45710,2262,13,16910,499,4587,3493,279,2262,315,279,45710,30,358,3358,387,6380,311,8819,279,8544,323,3645,1124,4184,311,701,8670,382,2746,499,1541,956,617,279,45710,2262,11,499,649,1101,25982,264,2678,44165,477,7664,279,2262,320,68,1326,2637,2027,836,11,5452,11,5099,6266,779,358,649,1456,311,1520,499,449,264,4689,6425,13],\"total_duration\":10398269800,\"load_duration\":65128500,\"prompt_eval_count\":132,\"prompt_eval_duration\":3814000000,\"eval_count\":92,\"eval_duration\":6507000000}\n",
      "Error processing chunk (JSON decode error): Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    url = input(\"Enter the URL to scrape: \")\n",
    "\n",
    "    # Extract page text from the URL\n",
    "    page_content = extract_text_from_url(url)\n",
    "    if page_content:\n",
    "        print(\"Page content extracted successfully.\")\n",
    "\n",
    "        # Extract reviews from the page content\n",
    "        reviews = extract_reviews_from_content(page_content)\n",
    "\n",
    "        if reviews:\n",
    "            print(f\"\\nFound {len(reviews)} reviews:\")\n",
    "            for idx, review in enumerate(reviews, 1):\n",
    "                print(f\"\\nReview {idx}:\")\n",
    "                print(f\"Author: {review['author']}\")\n",
    "                print(f\"Rating: {review.get('rating', 'N/A')} stars\")\n",
    "                print(f\"Review: {review['review']}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            # Ask user if they want to save the reviews\n",
    "            save = input(\"\\nWould you like to save the reviews to a file? (y/n): \")\n",
    "            if save.lower() == 'y':\n",
    "                filename = input(\"Enter filename (default: reviews.json): \").strip() or \"reviews.json\"\n",
    "                save_reviews(reviews, filename)\n",
    "                print(f\"Reviews saved to {filename}\")\n",
    "        else:\n",
    "            print(\"No reviews found.\")\n",
    "    else:\n",
    "        print(\"Failed to extract page content.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
